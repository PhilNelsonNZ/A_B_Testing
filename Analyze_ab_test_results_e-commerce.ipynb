{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results for e-Commerce website\n",
    "\n",
    "This project uses various statistical techniques to do A/B Testing on an e-commerce website.  The experiment is conducted on two versions of a web page \"new_page\" and \"old_page\".\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "This study creates an A/B test for an e-commerce website.  The purpose of this test is to decide if the \"new_page\" should be adopted or if the \"old_page\" should be kept.\n",
    "\n",
    "A/B Testing is a way to compare two versions of a web page or application that enables you to determine which one performs better. It is one of the easiest ways to analyze an application or a web page and determine which version is better in terms of conversions. \n",
    "\n",
    "A conversion is defined as an action that's counted when someone interacts with your ad (for example, clicks a text ad or views a video ad) and then takes an action that you've defined as valuable to your business, such as an online purchase or a call to your business from a mobile phone.\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability\n",
    "\n",
    "Import the libraries for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#We are setting the seed to assure you get the same answers on quizzes as we set up\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` This section reads in the `ab_data.csv` data and stores it in `df`.  \n",
    "\n",
    "a. Read in the dataset and take a look at the top few rows here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ab_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Use the below cell to find the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294478"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] #number of rows of df dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. The number of unique users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_usrs =(df.user_id.unique()) #unique users in df dataset\n",
    "len(unique_usrs) #number of unique users in the df dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d. The proportion of users converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12126269856564711"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converted users without dropping the duplicates - note this is not the correct value as it includes duplicates\n",
    "converted_users = sum(df.converted == 1)/len(unique_usrs)\n",
    "converted_users #proportion of users converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 290584 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      "user_id         290584 non-null int64\n",
      "timestamp       290584 non-null object\n",
      "group           290584 non-null object\n",
      "landing_page    290584 non-null object\n",
      "converted       290584 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 13.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dfU = df.drop_duplicates('user_id')\n",
    "dfU.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1195695564793657"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converted users dropping the duplicates\n",
    "converted_usersU = sum(dfU.converted == 1)/len(unique_usrs)\n",
    "converted_usersU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e. The number of times the `new_page` and `treatment` don't line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For the dataset with duplicates\n",
    "df.head()\n",
    "dfT = df[((df['group'] == 'treatment'))] #filter for all rows where 'group' == 'treatment'\n",
    "dfT.shape[0] #147276 rows\n",
    "dfTN = dfT[((dfT['landing_page'] == 'new_page'))] #filter again for all rows where 'landing_page' == 'new_page'\n",
    "dfTN.shape[0]\n",
    "\n",
    "treat_new_page = df.loc[(df['group'] == 'treatment') & (df['landing_page'] == 'new_page')]\n",
    "control_new_page = df.loc[(df['group'] == 'control') & (df['landing_page'] == 'new_page')]\n",
    "#dfTN[((dfTN['group'] == 'treatment') == (dfTN['landing_page'] == 'new_page')) == False].shape[0]\n",
    "treat_or_new = df.loc[(df['group'] == 'treatment') != (df['landing_page'] == 'new_page')]\n",
    "treat_or_new.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Do any of the rows have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna == True #there are no missing values in this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` For the rows where **treatment** is not aligned with **new_page** or **control** is not aligned with **old_page**, we cannot be sure if this row truly received the new or old page.\n",
    "\n",
    "a. To remove inconsistent data this section creates a new dataset that only includes data that is relevant to the analsis:\n",
    "\n",
    "**df2T** is a new dataset that only includes rows where 'group' == 'treatment' and 'landing_page' == 'new_page'\n",
    "\n",
    "**df2C** is a new dataset that only includes rows where 'group' == 'control' and 'landing_page' == 'old_page'\n",
    "\n",
    "**dfTC** and **df2C** are concatenated into a new dataframe stored as **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 290585 entries, 2 to 294476\n",
      "Data columns (total 5 columns):\n",
      "user_id         290585 non-null int64\n",
      "timestamp       290585 non-null object\n",
      "group           290585 non-null object\n",
      "landing_page    290585 non-null object\n",
      "converted       290585 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 13.3+ MB\n"
     ]
    }
   ],
   "source": [
    "###confirm all the possible values of the column 'group' and 'landing_page'\n",
    "df.group.unique()  # the only values are array(['control', 'treatment'], dtype=object)\n",
    "df.landing_page.unique() #array(['old_page', 'new_page'], dtype=object)\n",
    "\n",
    "df2T = df[(df['group'] == 'treatment') & (df['landing_page'] == 'new_page')]\n",
    "df2C = df[(df['group'] == 'control') & (df['landing_page'] == 'old_page')]\n",
    "#df2T.info() #145311\n",
    "#df2C.info() #145274\n",
    "df2 = pd.concat([df2T,df2C])\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the correct rows were removed - this should be 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` This section confirms and removes duplicate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many unique **user_id**s are in **df2**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2.user_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b. There is one **user_id** repeated in **df2**.  The following code identifies the duplicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2.user_id.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is the row information for the repeat **user_id**? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2.user_id == 773192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Remove **one** of the rows with a duplicate **user_id** and keep the dataframe as **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop([2893])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` This section uses **df2** calculate the probability of conversion for the null hypothesis.  Then it examines the probability of conversion of the **control** and **treatment** groups in the data.  It then examines the percentage of uses with **new_page** and **old_page**.  It then examines if based on the actual data is there sufficient evidence that the new_page leads to more conversions.\n",
    "\n",
    "a. What is the probability of an individual converting regardless of the page they receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df2.converted == 1)/df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Given that an individual was in the `control` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_new_pageUU = df2.loc[(df2['group'] == 'control')]\n",
    "len(control_new_pageUU[(control_new_pageUU['converted']==1)] )/ control_new_pageUU.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treat_new_pageUU = df2.loc[(df2['group'] == 'treatment')]\n",
    "len(treat_new_pageUU[(treat_new_pageUU['converted']==1)] )/ treat_new_pageUU.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is the probability that an individual received the new page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2[(df2['landing_page'] == 'new_page')])/df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. This section considers the results from a. through d. above, and explains below if there is sufficient evidence to say that the new treatment page leads to more conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0015782389853555567"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treat_new_pageUU = df2.loc[(df2['group'] == 'treatment')]\n",
    "new_conversion_rate = len(treat_new_pageUU[(treat_new_pageUU['converted']==1)] )/ treat_new_pageUU.shape[0] #0.11880806551510564\n",
    "control_new_pageUU = df2.loc[(df2['group'] == 'control')]\n",
    "old_conversion_rate = len(control_new_pageUU[(control_new_pageUU['converted']==1)] )/ control_new_pageUU.shape[0] #0.1204\n",
    "observed_diff = new_conversion_rate - old_conversion_rate #-0.0015782389853555567\n",
    "observed_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this data is a large sample it provides valuable information which seems to suggest that the new page does not lead to significantly more conversions with a very small observed difference of -0.00158, assuming the sample is representative of the popoulation.  Internet data is known to fluctuate during time of day and day of week and the sample should be examined to see if it is representative of all activity on the website and minimises the chance of sample bias.   It is a good practice to do a statistical analysis to determine if there is sufficient evidence that the new page treatment leads to more conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "`1.` For now lets consider making the decision just based on all the sample data in Part I.  If we assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%.  The hypotheses are stated in terms of words below which are the converted rates for the old and new pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null hypothesis:** new page is worse than or equal to the old page in conversion rate (type 1 error rate of 5%)\n",
    "\n",
    "**Alternate hypothesis:** new page is better than old page in conversion rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, assume they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n",
    "\n",
    "A sample size is used that is equal to the ones in **ab_data.csv**.  <br><br>\n",
    "\n",
    "This section performs the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. The **convert rate** for $p_{new}$ under the null is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the converted rate in ab_data.csv regardless of the page\n",
    "df2_convert = sum(df2.converted == 1)/len(df2)\n",
    "pnew = df2_convert\n",
    "pnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The **convert rate** for $p_{old}$ under the null is: <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pold = df2_convert\n",
    "pold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. $n_{new}$ is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnew = len(df2.loc[(df2['landing_page'] == 'new_page')])#updated\n",
    "nnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. $n_{old}$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nold = len(df2.loc[(df2['landing_page'] == 'old_page')])\n",
    "nold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. This section simulates $n_{new}$ transactions with a convert rate of $p_{new}$ under the null.  It stores these $n_{new}$ 1's and 0's in **new_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11737664303902003"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_new = dfU.loc[(dfU['landing_page'] == 'new_page')]\n",
    "#df_new.shape\n",
    "nnew_sim = df2.loc[(df2['landing_page'] == 'new_page')].sample(nnew,replace = True)\n",
    "new_page_converted = nnew_sim.converted\n",
    "new_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. This section simulates $n_{old}$ transactions with a convert rate of $p_{old}$ under the null.  It stores these $n_{old}$ 1's and 0's in **old_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12061346145903602"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_old = dfU.loc[dfU['landing_page'] == 'old_page']\n",
    "#df_old.shape\n",
    "nold_sim = df2.loc[(df2['landing_page'] == 'old_page')].sample(nold,replace = True)\n",
    "old_page_converted = nold_sim.converted\n",
    "old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. This section finds $p_{new}$ - $p_{old}$ for the simulated values from part (e) and (f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0032368184200159966"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pnew is the probability of conversion on the new page\n",
    "\n",
    "#p_new_old = pp_new - pp_old\n",
    "pp_new = new_page_converted.mean()\n",
    "pp_old = old_page_converted.mean()\n",
    "p_new_old = pp_new - pp_old\n",
    "#delta1\n",
    "p_new_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. This section simulates 10,000 $p_{new}$ - $p_{old}$ values using this same process similar to the ones calculated in parts **a. through g.** above.  The 10,000 values are stored in a numpy array called **p_diffs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diffs = np.random.binomial(nnew, pnew, 10000)*1.0/nnew - np.random.binomial(nold, pold, 10000)*1.0/nold\n",
    "#binomial above from numpy is much more computationally efficient than the loop below and is calculated more efficiently\n",
    "\n",
    "# When possible, it is always more computationally efficient to use numpy built-in operations over explicit for loops. \n",
    "# The short reason is that numpy-based operations attack a computational problem based on vectors by computing large chunks \n",
    "# simultaneously. Additionally, using loops to simulate 10000 can take a considerable amount of time vs using numpy\n",
    "# https://softwareengineering.stackexchange.com/questions/254475/how-do-i-move-away-from-the-for-loop-school-of-thought\n",
    "# Essentially, we are applying the null proportion to the total size of each page using the binomial distribution. \n",
    "\n",
    "#### This alternative approach took a long time to run  :-(\n",
    "#p_diffs = []\n",
    "\n",
    "#for _ in range(10000):\n",
    "#    sim_df = df2.sample(df2.shape[0], replace = True)\n",
    "#    new_conv_rate = sim_df.query(\"group == 'treatment'\")['converted'].mean()\n",
    "#    old_conv_rate = sim_df.query(\"group == 'control'\")['converted'].mean()\n",
    "#    p_diffs.append(new_conv_rate-old_conv_rate)\n",
    "#    null_vals = np.random.normal(0, np.array(p_diffs).std(), len(p_diffs))\n",
    "#    print (\"Proportion Greater : {}\".format((np.array(null_vals) > observed_diff).mean()), end = \"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Plot a histogram of the **p_diffs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print histogram\n",
    "plt.hist(p_diffs);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j. What proportion of the **p_diffs** are greater than the actual difference observed in **ab_data.csv**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9108"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_diff = np.array(p_diffs)\n",
    "proportion = actual_diff > observed_diff\n",
    "sum(proportion)/len(p_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k. This section explains what is computed in part **j.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is computed in j.  When you perform a hypothesis test in statistics, a p-value helps you determine the significance of your results. A large p-value (> 0.05) indicates weak evidence against the null hypothesis so you fail to reject the null hypothesis. The p-value is 0.91 of the p_diffs are greater than the observed data in ab_data.csv.  This value means we fail to reject the null hypothesis that new page is worse than or equal to the old page in conversion rate (type 1 error rate of 5%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l. We can also use a built-in function from statsmodels.api to achieve similar results.  Using the built-in is easier to code, however the above portions walkthrough the ideas that are critical to think correctly about statistical significance for A/B testing. The code below calculates the number of conversions for each page, as well as the number of individuals who received each page. For this model `n_old` and `n_new` refer the the number of rows associated with the old page and new pages, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phil Nelson\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "convert_old = df2.query(\"landing_page == 'old_page' and converted == 1\").shape[0]\n",
    "convert_new = df2.query(\"landing_page == 'new_page' and converted == 1\").shape[0]\n",
    "n_old = len(df2.loc[(df2['landing_page'] == 'old_page')])\n",
    "n_new = len(df2.loc[(df2['landing_page'] == 'new_page')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m. This section uses `sm.stats.proportions_ztest` to compute the test statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17489"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm number of conversions for p_old\n",
    "convert_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17264"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3109241984234394, 0.9050583127590245)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "z_score, p_value = sm.stats.proportions_ztest([convert_new,convert_old], [n_new, n_old],alternative='larger') #larger one-sided\n",
    "z_score, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n. This section discusses the z-score and p-value you computed in the previous section and explains what this means for the conversion rates of the old and new pages.  It also examines if they agree with the findings in parts **j.** and **k.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z-score indicates how many standard deviations away from the mean which is -1.3  This means that for the p-value of 0.9 indicates very weak evidence against the null hypothesis so it fails to reject the null hypothesis. The p-value is not enough to reject the null hypothesis as found using the sampling method.\n",
    "\n",
    "The z value indicates that the mean are different by -1.3 std deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "`1.` This section shows that the results acheived in the previous A/B test can also be acheived by performing regression.<br><br>\n",
    "\n",
    "a. Since each row is in two states i.e. either a \"conversion\" or \"no conversion\" logistic regression is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is used to predict anything with only two outcomes (in this case conversion or no conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. This section uses **statsmodels** to fit the regression model specified in part **a.** to see if there is a significant difference in conversion based on which page a customer receives.  First a column is created for the intercept, and then a dummy variable column is created for which page (i.e. new_page or old_page) each user received.  Also an **ab_page** column is added, which is 1 when an individual receives the **treatment** and 0 if **control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This section prepares the dataset into a binary model which can then be used for logistic regression modelling\n",
    "\n",
    "df2['intercept'] = 1\n",
    "df2['ab_page'] = pd.get_dummies(df2['group'])['treatment']\n",
    "df2['ab_page'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use **statsmodels** to import your regression model.  Instantiate the model, and fit the model using the two columns you created in part **b.** to predict whether or not an individual converts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats # to fix bug \"AttributeError: module 'scipy.stats' has no attribute 'chisqprob'\"\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df) #bug fix  \"AttributeError: module 'scipy.stats' has no attribute...\"\n",
    "\n",
    "logit_mod = sm.Logit(df2['converted'], df2[['intercept','ab_page']])\n",
    "results = logit_mod.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Provide the summary of your model below, and use it as necessary to answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290582</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 18 Jul 2018</td> <th>  Pseudo R-squ.:     </th>  <td>8.077e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>14:54:57</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.1899</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290582\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Wed, 18 Jul 2018   Pseudo R-squ.:               8.077e-06\n",
       "Time:                        14:54:57   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.1899\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. The p-value associated with **ab_page** is discussed and show above and it differs from the value found in **Part II**?<br><br>  The explanation for this is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null hypothesis:** new page is equal to the old page in conversion rate (type 1 error rate of 5%) <br>\n",
    "**Alternate hypothesis:** new page is better than or worse than the old page in conversion rate. <br>\n",
    "This is a two sided hypothesis test whereas in section II it is a one-sided hypothesis test.\n",
    "The p-value of .19 differs the value found in part II (0.905).  The two sided test of 0.19 can be compared with the one-sided test by comparing 1 - 0.19/2 = 1 - .095 = 0.905 which is equivalent.\n",
    "Note that the R squared value is very small and so the logistic regression does not fit the data well.  None of the results provide enough evidence to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Now, you are considering other things that might influence whether or not an individual converts.  Discuss why it is a good idea to consider other factors to add into your regression model.  Are there any disadvantages to adding additional terms into your regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key aspects to modelling is finding the factors or components that are most related or associated to the thing that is trying to be predicted this can be done using techniques like forward selection or backward selection. Domain knowledge and careful systematic questions can steer the analyst to consider other factors inside the dataset (ratio) or external data that may be able to be associated with the data.  However, simple models with fewer factors are easier for analysts and the business to understand and take actions which may be advantageous.  Models with more factors are more complex and sometimes may be difficult for the business to understand or take advantage of.  The assumptions for the model should be verified to determine whether they meet the precondition assumptions for modeling (i.e. independence and linearity) and can be verified using other analysis (e.g. residual plot, normal probability plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Now along with testing if the conversion rate changes for different pages this section adds an effect based on which country a user lives. The **countries.csv** dataset is read and merged together with the df2 datasets on the approporiate rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = pd.read_csv('./countries.csv')\n",
    "df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "US    203619\n",
       "UK     72466\n",
       "CA     14499\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.country.value_counts() # confirm how many of each country are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>group_treatment</th>\n",
       "      <th>country_UK</th>\n",
       "      <th>country_US</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834778</th>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928468</th>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822059</th>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711597</th>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710616</th>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          timestamp  converted  intercept  ab_page  \\\n",
       "user_id                                                              \n",
       "834778   2017-01-14 23:08:43.304998          0          1        0   \n",
       "928468   2017-01-23 14:44:16.387854          0          1        1   \n",
       "822059   2017-01-16 14:04:14.719771          1          1        1   \n",
       "711597   2017-01-22 03:14:24.763511          0          1        0   \n",
       "710616   2017-01-16 13:14:44.000513          0          1        1   \n",
       "\n",
       "         group_treatment  country_UK  country_US  \n",
       "user_id                                           \n",
       "834778                 0           1           0  \n",
       "928468                 1           0           1  \n",
       "822059                 1           1           0  \n",
       "711597                 0           1           0  \n",
       "710616                 1           1           0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the necessary dummy variables\n",
    "df_new_log = pd.get_dummies(df_new,columns = ['landing_page', 'group','country']) #create dummies\n",
    "df_new_log = df_new_log.drop(['landing_page_old_page','landing_page_new_page','group_control','country_CA'],axis=1) #drop unnecessary columns\n",
    "df_new_log['intercept'] = 1\n",
    "df_new_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Though the individual factors of country and page on conversion have been created, lets now examine the interaction between page and country to see if there significant effects on conversion.  Additional columns are added and fit the new model.  \n",
    "\n",
    "The summary results and conclusions based on the results are given in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290578</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     5</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 18 Jul 2018</td> <th>  Pseudo R-squ.:     </th>  <td>3.482e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>14:55:09</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.1920</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>       <td>   -2.0040</td> <td>    0.036</td> <td>  -55.008</td> <td> 0.000</td> <td>   -2.075</td> <td>   -1.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>group_treatment</th> <td>   -0.0674</td> <td>    0.052</td> <td>   -1.297</td> <td> 0.195</td> <td>   -0.169</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_US</th>      <td>    0.0175</td> <td>    0.038</td> <td>    0.465</td> <td> 0.642</td> <td>   -0.056</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_UK</th>      <td>    0.0118</td> <td>    0.040</td> <td>    0.296</td> <td> 0.767</td> <td>   -0.066</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK_ind_ab_page</th>  <td>    0.0783</td> <td>    0.057</td> <td>    1.378</td> <td> 0.168</td> <td>   -0.033</td> <td>    0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US_ind_ab_page</th>  <td>    0.0469</td> <td>    0.054</td> <td>    0.872</td> <td> 0.383</td> <td>   -0.059</td> <td>    0.152</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290578\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Wed, 18 Jul 2018   Pseudo R-squ.:               3.482e-05\n",
       "Time:                        14:55:09   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.1920\n",
       "===================================================================================\n",
       "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "intercept          -2.0040      0.036    -55.008      0.000      -2.075      -1.933\n",
       "group_treatment    -0.0674      0.052     -1.297      0.195      -0.169       0.034\n",
       "country_US          0.0175      0.038      0.465      0.642      -0.056       0.091\n",
       "country_UK          0.0118      0.040      0.296      0.767      -0.066       0.090\n",
       "UK_ind_ab_page      0.0783      0.057      1.378      0.168      -0.033       0.190\n",
       "US_ind_ab_page      0.0469      0.054      0.872      0.383      -0.059       0.152\n",
       "===================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fit Logistic Regression Model And Obtain the Results\n",
    "\n",
    "df_new_log['UK_ind_ab_page'] = df_new_log['country_UK']*df_new_log['group_treatment'] #add new column UK_ind_ab_page\n",
    "df_new_log['US_ind_ab_page'] = df_new_log['country_US']*df_new_log['group_treatment'] #add new column US_ind_ab_page\n",
    "logit_modC = sm.Logit(df_new_log['converted'], df_new_log[['intercept', 'group_treatment', 'country_US', 'country_UK', 'UK_ind_ab_page', 'US_ind_ab_page']])\n",
    "\n",
    "\n",
    "resultsC = logit_modC.fit()\n",
    "resultsC.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional factors of UK_ind_ab_page and US_ind_ab_page improve the R squared model slightly but not significantly (compare with below).  It does not change the conclusion or change the p-value which indicates that the null hypothesis should not be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 18 Jul 2018</td> <th>  Pseudo R-squ.:     </th>  <td>2.323e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>14:55:10</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.1760</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>       <td>   -2.0300</td> <td>    0.027</td> <td>  -76.249</td> <td> 0.000</td> <td>   -2.082</td> <td>   -1.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>group_treatment</th> <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_US</th>      <td>    0.0408</td> <td>    0.027</td> <td>    1.516</td> <td> 0.130</td> <td>   -0.012</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_UK</th>      <td>    0.0506</td> <td>    0.028</td> <td>    1.784</td> <td> 0.074</td> <td>   -0.005</td> <td>    0.106</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Wed, 18 Jul 2018   Pseudo R-squ.:               2.323e-05\n",
       "Time:                        14:55:10   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.1760\n",
       "===================================================================================\n",
       "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "intercept          -2.0300      0.027    -76.249      0.000      -2.082      -1.978\n",
       "group_treatment    -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "country_US          0.0408      0.027      1.516      0.130      -0.012       0.093\n",
       "country_UK          0.0506      0.028      1.784      0.074      -0.005       0.106\n",
       "===================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The model without the new factors of UK_ind_ab_page and US_ind_ab_page included for comparison with the model above\n",
    "logit_modB = sm.Logit(df_new_log['converted'], df_new_log[['intercept', 'group_treatment', 'country_US', 'country_UK']])\n",
    "\n",
    "\n",
    "resultsB = logit_modB.fit()\n",
    "resultsB.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Conclusions\n",
    "\n",
    "The conclusion is that with and without country factors these two logistic regression models both give small pseudo R squared values that the models do not predict the variance well and so no conclusion can be drawn.  In addition the p-values do not indicate the null hypothesis should be rejected.  Based on these results there is no evidence the new page is getting more conversions than the old page.  As discussed earlier it is important to determine if the sampling has been random as time of day and day of week effects do occur with website internet traffic.  For further work it should ideally be confirmed that there is no time period bias in the control vs. treatment data.\n",
    "\n",
    "Sanity check.  From a practical absolute numbers perspective the 0.1204 conversion rate for new pages is quite close to the 0.1196 conversion rate for old pages.  0.1204/0.1196 = 1.006 which is a small difference in conversion rate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
